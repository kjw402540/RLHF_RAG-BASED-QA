# -*- coding: utf-8 -*-

# ============================================================
# âš™ï¸ Step 3: PPO (Reinforcement Learning) - ì•ˆì „ ë²„ì „
# ============================================================

import os
import json
import random
import torch
from datasets import Dataset
from trl import PPOTrainer, PPOConfig, create_reference_model
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSequenceClassification,
    BitsAndBytesConfig
)
from peft import PeftModel, LoraConfig

# ============================================================
# 1. ë°ì´í„° ìƒ˜í”Œ ë¹„ìœ¨ ì„¤ì •
# ============================================================
DATA_SAMPLE_RATIO = 0.1

def sample_data(data_list, ratio=0.05, seed=42):
    random.seed(seed)
    n_sample = max(1, int(len(data_list) * ratio))
    return random.sample(data_list, n_sample)

# ============================================================
# 2. JSON íŒŒì¼ ë¡œë“œ í•¨ìˆ˜
# ============================================================
def load_custom_json(path):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
        return data.get("data_info", [])

# ============================================================
# 3. ë°ì´í„° ë¡œë“œ ë° ìƒ˜í”Œë§
# ============================================================
sft_data = sample_data(load_custom_json("./data/train/RLHF_train/SFT.json"), DATA_SAMPLE_RATIO)
rm_data = sample_data(load_custom_json("./data/train/RLHF_train/RM.json"), DATA_SAMPLE_RATIO)
ppo_data = sample_data(load_custom_json("./data/train/RLHF_train/PPO.json"), DATA_SAMPLE_RATIO)

# ============================================================
# 0ï¸âƒ£ ê²½ë¡œ ì„¤ì •
# ============================================================
output_dir = "./weight/output_rlhf/llama2"
MODEL_NAME = "beomi/llama-2-ko-7b"
SFT_MODEL_PATH = f"{output_dir}/sft_lora"
RM_MODEL_PATH = f"{output_dir}/rm_lora"
PPO_SAVE_PATH = f"{output_dir}/ppo_lora"

# ============================================================
# 1ï¸âƒ£ Tokenizer ë¡œë“œ
# ============================================================
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)
# ============================================================
# 3ï¸âƒ£ SFT ëª¨ë¸ ë¡œë“œ (Policy)
# ============================================================
print("ğŸ“¥ SFT LoRA ëª¨ë¸ ë¡œë“œ ì¤‘...")
base_policy = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config=bnb_config,
    device_map="auto"
)
policy_model = PeftModel.from_pretrained(base_policy, SFT_MODEL_PATH)
policy_model = policy_model.merge_and_unload()
print("âœ… Policy ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# ============================================================
# 4ï¸âƒ£ Reference ëª¨ë¸ ìƒì„±
# ============================================================
ref_model = create_reference_model(policy_model)
print("âœ… Reference ëª¨ë¸ ìƒì„± ì™„ë£Œ")

# ============================================================
# 5ï¸âƒ£ RM ëª¨ë¸ ë¡œë“œ (Reward Model) ë° value ëª¨ë¸
# ============================================================
print("ğŸ“¥ RM LoRA ëª¨ë¸ ë¡œë“œ ì¤‘...")
base_rm = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=1,
    quantization_config=bnb_config,
    device_map="auto"
)
reward_model = PeftModel.from_pretrained(base_rm, RM_MODEL_PATH)
print("âœ… Reward ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

value_model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=1,
    quantization_config=bnb_config,
    device_map="auto"
)

# ============================================================
# 6ï¸âƒ£ PPO ë°ì´í„°ì…‹ êµ¬ì„±
# ============================================================
# query ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³  Dataset ìƒì„±
texts = [
     d.get("question", "").strip()
     for d in ppo_data
     if "question" in d and d["question"].strip() != ""
]

# 3ï¸âƒ£ Tokenize
tokenized = tokenizer(
    texts,
    truncation=True,
    padding="max_length",
    max_length=256,  # í•„ìš”ì— ë”°ë¼ ì¡°ì • (ë³´í†µ 128~512)
    return_tensors="pt",  # PyTorch í…ì„œë¡œ ë°˜í™˜
)

# 4ï¸âƒ£ Datasetìœ¼ë¡œ ë³€í™˜
ppo_dataset = Dataset.from_dict({
    "input_ids": tokenized["input_ids"],
    "attention_mask": tokenized["attention_mask"],
})

# ============================================================
# collate_fn (ë°©ì–´ì )
# ============================================================
def collate_fn(batch):
    if len(batch) == 0:
        raise ValueError("Empty batch passed to collate_fn")

    try:
        input_ids = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(b["input_ids"], dtype=torch.long) for b in batch],
            batch_first=True, padding_value=tokenizer.pad_token_id
        )
        attention_mask = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(b["attention_mask"], dtype=torch.long) for b in batch],
            batch_first=True, padding_value=0
        )
    except Exception as e:
        print("âŒ Collate failed on batch:")
        print(batch)
        raise e

    return {"input_ids": input_ids, "attention_mask": attention_mask}


# ============================================================
# 7ï¸âƒ£ PPO ì„¤ì •
# ============================================================
ppo_config = PPOConfig(
    batch_size=2,
    mini_batch_size=1,
    learning_rate=1.4e-5,
    report_to="none",
    num_sample_generations=0
)

# ============================================================
# 8ï¸âƒ£ PPO Trainer ì •ì˜
# ============================================================
ppo_trainer = PPOTrainer(
    model=policy_model,
    ref_model=ref_model,
    value_model=value_model,
    reward_model=reward_model,
    processing_class=tokenizer,
    train_dataset=ppo_dataset,
    data_collator=collate_fn,
    args=ppo_config,
)

# ============================================================
# 9ï¸âƒ£ PPO í•™ìŠµ ë£¨í”„
# ============================================================
import time
print("ğŸš€ PPO í•™ìŠµ ì‹œì‘...")
start_time = time.time()

ppo_trainer.train()

end_time = time.time()
print(f"âœ… PPO í•™ìŠµ ì™„ë£Œ - Total Time: {end_time - start_time:.2f}s")

# ============================================================
# ğŸ”Ÿ ëª¨ë¸ ì €ì¥
# ============================================================
ppo_trainer.save_model(PPO_SAVE_PATH)
tokenizer.save_pretrained(PPO_SAVE_PATH)
print(f"âœ… PPO ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {PPO_SAVE_PATH}")