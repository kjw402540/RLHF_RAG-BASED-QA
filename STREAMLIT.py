

%%writefile app.py
import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import os

# --- ì„¤ì • (ì´ì „ ì½”ë“œì—ì„œ ê°€ì ¸ì˜´) ---
# ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
# ì˜ˆ: Drive ë§ˆìš´íŠ¸ í›„ 'drive/MyDrive/my_qa_model' í´ë”ì— ì €ì¥í–ˆì„ ê²½ìš°
BASE_MODEL_NAME = "google/gemma-2-2b"
ADAPTER_PATH = "/content/drive/MyDrive/ai2/sft_lora" # Colab ê²½ë¡œë¥¼ ê°€ì •


# GPU ì‚¬ìš©ì„ ìœ„í•´ device_map="auto" ì„¤ì •
DEVICE_MAP = "auto"
DTYPE = torch.bfloat16
OFFLOAD_DIR = "/tmp/model_offload"
# ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥

# QA í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •
SYSTEM_PROMPT = "ë‹¹ì‹ ì€ ì‚¬ìš©ì í•™ìŠµì„ í†µí•´ íŠ¹ì • ë„ë©”ì¸ì— ëŒ€í•´ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ìœ ëŠ¥í•œ QA ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”."
QA_PROMPT_TEMPLATE = """
{history}
ì‚¬ìš©ìì˜ ì§ˆë¬¸: {prompt}

ëª¨ë¸ì˜ ë‹µë³€:"""

# --- ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ---
@st.cache_resource
def load_model():
    """LoRA (PEFT) ì–´ëŒ‘í„°ê°€ ì ìš©ëœ 4ë¹„íŠ¸ ì–‘ìí™” ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    # 1. ì–‘ìí™” ì„¤ì • (4ë¹„íŠ¸)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
    )

    # 2. offload í´ë” ìƒì„±
    if not os.path.exists(OFFLOAD_DIR):
        os.makedirs(OFFLOAD_DIR)

    # 3. ê¸°ë°˜ ëª¨ë¸(Base Model) ë¡œë“œ
    st.info(f"ê¸°ë°˜ ëª¨ë¸ ({BASE_MODEL_NAME}) 4ë¹„íŠ¸ ì–‘ìí™” ë¡œë“œ ì¤‘...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        quantization_config=bnb_config,
        device_map=DEVICE_MAP,
        torch_dtype=DTYPE,
        offload_folder=OFFLOAD_DIR,
    )
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL_NAME,
        add_eos_token=True
        )

    # 4. PEFT ì–´ëŒ‘í„° ë¡œë“œ ë° ë³‘í•©
    st.info(f"LoRA ì–´ëŒ‘í„° ({ADAPTER_PATH}) ì ìš© ì¤‘...")
    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_PATH,
    )
    # ì¶”ë¡  ëª¨ë“œë¡œ ì „í™˜
    model.eval()

    return tokenizer, model

# --- ëŒ€í™” íˆìŠ¤í† ë¦¬ í¬ë§¤íŒ… í•¨ìˆ˜ ---
def format_conversation_history(messages):
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ê¸° ìœ„í•œ ë¬¸ìì—´ë¡œ í¬ë§·í•©ë‹ˆë‹¤."""
    if not messages:
        return ""

    history_str = ""
    for msg in messages:
        if msg["role"] == "user":
            history_str += f"ì‚¬ìš©ìì˜ ì´ì „ ì§ˆë¬¸: {msg['content']}\n"
        elif msg["role"] == "assistant":
            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ëŠ” ëª¨ë¸ì˜ ë‹µë³€ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë‹µë³€ í…œí”Œë¦¿ì— ë§ì¶¤
            history_str += f"ëª¨ë¸ì˜ ì´ì „ ë‹µë³€: {msg['content']}\n"

    return history_str.strip()

# --- ì±—ë´‡ ë¡œì§ í•¨ìˆ˜ (ë©€í‹°í„´ ì§€ì›) ---
def generate_response(tokenizer, model, user_prompt, history_messages):
    """Gemma 2-2B ê¸°ë°˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë©€í‹°í„´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""

    # 1. íˆìŠ¤í† ë¦¬ í¬ë§·
    history = format_conversation_history(history_messages)

    # 2. ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„± (System Prompt + History + Current Prompt)
    full_prompt = f"{SYSTEM_PROMPT}\n\n{QA_PROMPT_TEMPLATE.format(history=history, prompt=user_prompt)}"

    # 3. í† í¬ë‚˜ì´ì§• ë° GPU ì´ë™
    inputs = tokenizer.encode(full_prompt, return_tensors="pt")
    inputs = inputs.to(model.device)

    # 4. ì‘ë‹µ ìƒì„±
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_new_tokens=256,
            do_sample=True,
            temperature=0.5,
            top_k=50,
            top_p=0.95,
            eos_token_id=tokenizer.eos_token_id
        )

    # 5. ì¶œë ¥ ë””ì½”ë”© ë° í”„ë¡¬í”„íŠ¸ ì œê±°
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œê±°í•˜ê³  ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    # ë³µì¡í•œ í…œí”Œë¦¿ êµ¬ì¡° ë•Œë¬¸ì— QA_PROMPT_TEMPLATE ë¶€ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ëƒ…ë‹ˆë‹¤.
    answer_prefix = QA_PROMPT_TEMPLATE.format(history=history, prompt=user_prompt).strip()

    if answer_prefix in response:
        # í…œí”Œë¦¿ ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        answer = response.split(answer_prefix, 1)[-1].strip()
        # "ëª¨ë¸ì˜ ë‹µë³€:" ë¶€ë¶„ ì œê±°
        answer = answer.replace("ëª¨ë¸ì˜ ë‹µë³€:", "").strip()
    else:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ì „ì²´ ì‘ë‹µì—ì„œ ì‹œìŠ¤í…œ/íˆìŠ¤í† ë¦¬ë§Œ ì œê±° ì‹œë„ (ê°„ë‹¨ ì²˜ë¦¬)
        answer = response.replace(SYSTEM_PROMPT, "").strip()

    # ğŸ’¡ "ì‚¬ìš©ìì˜ ì§ˆë¬¸:" íŒ¨í„´ë„ ì œê±°ë  ìˆ˜ ìˆë„ë¡ ìµœì¢… ì •ë¦¬
    if "ì‚¬ìš©ìì˜ ì§ˆë¬¸:" in answer:
        answer = answer.split("ì‚¬ìš©ìì˜ ì§ˆë¬¸:", 1)[0].strip()

    return answer

# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="Custom Gemma 2-2B ì±—ë´‡ (Streamlit)", layout="centered")

st.title("ğŸ¤– ê°“ê¸°ì„±ìœ¤ì˜ ì±—ë´‡")
st.caption("ì‚¬ìš©ì í•™ìŠµ ê¸°ë°˜ ë©€í‹°í„´ ëŒ€í™” ì§€ì›")

# --- ëª¨ë¸ ë¡œë“œ ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ---
with st.spinner("ì‚¬ìš©ì í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì¤‘..."):
    try:
        # ëª¨ë¸ ë¡œë“œ (st.cache_resource ì‚¬ìš©)
        tokenizer, model = load_model()
        st.success("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ê²½ë¡œ({BASE_MODEL_NAME})ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”! ìƒì„¸ ì˜¤ë¥˜: {e}")
        st.stop()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.caption("âš ï¸ **PEFT ëª¨ë¸ ì¶”ë¡  í™˜ê²½**")

    st.divider()

    # ìƒˆ ëŒ€í™” ì‹œì‘ ë²„íŠ¼
    if st.button("ğŸ”„ ìƒˆ ëŒ€í™” ì‹œì‘", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    st.divider()

    # í˜„ì¬ ëŒ€í™” í„´ ìˆ˜
    st.caption(f"ğŸ’¬ ëŒ€í™” í„´: {len(st.session_state.messages)//2}í„´")

# --- ë©”ì¸ í™”ë©´: ëŒ€í™” ê¸°ë¡ í‘œì‹œ ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ì‚¬ìš©ì ì…ë ¥ ë° ì‘ë‹µ ìƒì„± ---
if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):

    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„±
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                # generate_response í•¨ìˆ˜ì— í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ì„ ì „ë‹¬
                # ë§ˆì§€ë§‰ ë©”ì‹œì§€(í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€)ë¥¼ ì œì™¸í•œ ê¸°ë¡ë§Œ ì „ë‹¬í•´ì•¼ í•¨
                history_messages = st.session_state.messages[:-1]

                answer = generate_response(tokenizer, model, prompt, history_messages)

                st.markdown(answer)

            except Exception as e:
                answer = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                st.error(answer)

        # 3. ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": answer})

st.markdown("---")
with st.expander("â„¹ï¸ ëª¨ë¸ ì •ë³´"):
    st.markdown(f"**ê¸°ë°˜ ëª¨ë¸:** `{BASE_MODEL_NAME}`")
    st.markdown(f"**ì–´ëŒ‘í„° ê²½ë¡œ:** `{ADAPTER_PATH}`")
    st.markdown("ì´ ë´‡ì€ PEFT í•™ìŠµëœ Gemma ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.")

from pyngrok import ngrok
# ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
# --- ngrok ì„¤ì • ---
# ngrok íšŒì›ê°€ì…í•´ì„œ í† í° ë°›ì•„ì•¼í•¨
NGROK_AUTH_TOKEN = ""
ngrok.set_auth_token(NGROK_AUTH_TOKEN)
# Gemma2-2b ì‚¬ìš©ì„ ìœ„í•œ í—ˆê¹…í˜ì´ìŠ¤ í† í°
from huggingface_hub import login
login("")
# ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥
# Streamlit ì•± ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰)
get_ipython().system_raw('streamlit run app.py &')

# ngrok í„°ë„ ì—´ê¸°
print("Streamlit ì•±ì„ ìœ„í•œ ngrok í„°ë„ì„ ì—¬ëŠ” ì¤‘...")
try:
    # ğŸ’¡ ìˆ˜ì •: port='8501'ì„ addr='8501'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    public_url = ngrok.connect(addr='8501')

    print(f"\nğŸ‰ Streamlit ì•±ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ğŸ”— ì ‘ì† ì£¼ì†Œ: {public_url}")
    print("\nâš ï¸ ì´ ì£¼ì†Œë¥¼ í´ë¦­í•˜ì—¬ QA ë´‡ UIë¥¼ í™•ì¸í•˜ì„¸ìš”.")
except Exception as e:
    print(f"\nğŸš« ngrok í„°ë„ ìƒì„± ì‹¤íŒ¨: {e}")
    print("ngrok Auth Tokenì„ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")