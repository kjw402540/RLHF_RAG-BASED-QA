ğŸ§  RLHF-RAG-BASED-QA
Reinforcement Learning from Human Feedback (RLHF) ê¸°ë°˜ì˜ RAG(Retrieval-Augmented Generation) QA ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
Supervised Fine-Tuning(SFT) â†’ Reward Model(RM) â†’ PPO â†’ LoRA Merge ë‹¨ê³„ë¥¼ í†µí•´ Llama ê¸°ë°˜ ëª¨ë¸ì„ ì¸ê°„ í”¼ë“œë°±ì— ë§ê²Œ ì •ì œí•©ë‹ˆë‹¤.

ğŸ“‚ Project Structure
RLHF-RAG-BASED-QA/
â”œâ”€â”€ SFT.py # Supervised Fine-Tuning (ì§€ë„ í•™ìŠµ)
â”œâ”€â”€ RM.py # Reward Model í•™ìŠµ (ì‘ë‹µ í’ˆì§ˆ í‰ê°€ ëª¨ë¸)
â”œâ”€â”€ PPO.py # Reinforcement Learning fine-tuning (PPO ì•Œê³ ë¦¬ì¦˜)
â”œâ”€â”€ MERGE.py # LoRA ë³‘í•© ë° ìµœì¢… ëª¨ë¸ ì €ì¥
â””â”€â”€ README.md

ğŸš€ Pipeline Overview
SFT (Supervised Fine-Tuning)
Human-labeled datasetìœ¼ë¡œ ê¸°ë³¸ ì–¸ì–´ëª¨ë¸ì„ ì§€ë„í•™ìŠµ.
ëª¨ë¸ì´ ì£¼ì–´ì§„ ì§ˆë¬¸ì— ë” ì¼ê´€ì ì´ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.

RM (Reward Model)
SFT ëª¨ë¸ì´ ìƒì„±í•œ ì‘ë‹µ ì¤‘ â€œë” ì¸ê°„ì ì¸ ì‘ë‹µâ€ì„ ì‹ë³„í•  ìˆ˜ ìˆë„ë¡ ë³´ìƒëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.

PPO (Reinforcement Learning Fine-tuning)
PPO(Proximal Policy Optimization) ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´
ë³´ìƒëª¨ë¸ì˜ í”¼ë“œë°±ì„ ë°”íƒ•ìœ¼ë¡œ ì–¸ì–´ëª¨ë¸ì„ ê°•í™”í•™ìŠµí•©ë‹ˆë‹¤.

MERGE (LoRA Merge & Save)
LoRA ì–´ë¸í„°ë¥¼ ì›ë³¸ ëª¨ë¸ì— ë³‘í•©í•˜ì—¬ ìµœì¢… ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.

ğŸ§© Example Models
Base model: meta-llama/Llama-3.2-1B
Tokenizer: HuggingFace Transformers ê¸°ë°˜
Quantization: BitsAndBytes (4bit, nf4)

ğŸ“œ License
This project is for research and educational purposes.
All pretrained models are from publicly available sources (e.g., Hugging Face).

